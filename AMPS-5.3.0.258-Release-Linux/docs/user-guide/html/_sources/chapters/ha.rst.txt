.. index:: High availability, Replication, Transactions, Availability,

.. _#ug-ha:

25. Replication and High Availability
=====================================

This chapter discusses the support that AMPS provides for replication,
and how AMPS features help to build systems that provide high
availability.

.. include:: ./overview.inc
High Availability Scenarios
---------------------------

You design your high availability strategy to meet the needs of your
application, your business, and your network. This section describes
commonly-deployed scenarios for high availability.

Failover Scenario
^^^^^^^^^^^^^^^^^

One of the most common scenarios is for two AMPS instances to replicate
to each other. This replication is synchronous, so that both instances
persist a message before AMPS acknowledges the message to the publisher.
This makes a hot-hot pair. In the figure below, any messages published
to ``important_topic`` are replicated across instances, so both
instances have the messages for ``important_topic``.

.. image:: ../../common/chapters/ha/svg/Hot-Hot-Replication.svg

Notice that, because AMPS replication is peer-to-peer, clients can
connect to either instance of AMPS when both are running. Further,
messages can be published to either instance of AMPS and be replicated
to the other instance. In this case, clients are configured with the
addresses of both AMPS instances.

In this case, clients are configured with Instance 1 and Instance 2 as
equivalent server addresses. If a client cannot connect to one instance,
it tries the other. Because both instances contain the same messages for
``important_topic``, there is no functional difference in which instance
a client connects to. Because these instances replicate to each other,
AMPS can optimize this to a single connection. Two connections are shown
in the diagram to demonstrate the required configuration.

Because these instances are intended to be equivalent message sources
(that is -- a client may fail over from one instance to another instance),
these instances are configured to use ``sync`` acknowledgement to publishers.
What that means is that, when a message is published to one of these instances,
that instance does not acknowledge the message to the publisher as persisted
until both instances have written the message to disk (although the message can
be delivered to subscribers once it is peristed locally).

Geographic Replication
^^^^^^^^^^^^^^^^^^^^^^

AMPS is well suited for replicating messages to different regions, so
clients in those regions are able to quickly receive and publish
messages to a local instance. In this case, each region replicates all
messages on the topic of interest to the other two regions. A variation
on this strategy is to use a region tag in the content, and use content
filtering so that each replicates messages intended for use in the other
regions or worldwide.

.. image:: ../../common/chapters/ha/svg/GeoRepl.svg

For this scenario, an AMPS instance in each region replicates to an
instance in the two other regions. For the best performance, replication
between the regions is asynchronous, so that once an instance in one
region has persisted the message, the message is acknowledged back to
the publisher.

In this case, clients in each region connect *only* to the AMPS instance in
that region. Bandwidth within regions is conserved, because each message
is replicated once to the region, regardless of how many subscribers in
that region will receive the message. Further, publishers are able to
publish the message once to a local instance over a relatively fast
network connection rather than having to publish messages multiple times
to multiple regions.

To configure this scenario, the AMPS instances in each region are
configured to forward messages to known instances in the other two
regions.

Geographic Replication with High Availability
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Combining the first two scenarios allows your application to distribute
messages as required and to have high availability in each region. This
involves having two or more servers in each region, as shown in the
figure below.

.. image:: ../../common/chapters/ha/svg/Replication-Scenario-with-HA.svg

Each region is configured as a group. Within each group, the instances
replicate to each other using ``sync`` acknowledgements, to ensure that
publishers can fail over between the instances. Because a client in a
given region does not connect to a server outside the region, we can
configure the replication links between the regions to use ``async``
acknowledgement, which could *potentially* reduce the amount of time
that an application publishing to AMPS must store messages before receiving
acknowledgement that the messages are persisted.

The figure below shows the expanded detail of the configuration for these servers.

.. image:: ../../common/chapters/ha/svg/Chicago-Detail.svg
   :width: 60.0%


The instances in each region are configured to be part of a group for
that region. Within a region, the instances synchronously replicate to
each other, and asynchronously replicate to instances at each remote
site. The instances use the replication downgrade action to ensure that
message publishing continues in the event that one of the instances goes
offline. As with all connections where instances replicate to each
other, this replication is configured as one connection in each
direction, although AMPS may optimize this to a single replication
connection.

Each instance at a site provides passthrough replication from the other
sites to local instances, so that once a message arrives at the site, it
is replicated to the other instances at the local site. The remote sites
are configured in the same way. This configuration balances
fault-tolerance and performance.

Each instance at a site replicates to the remote sites. The instance
specifies one ``Destination`` for each remote site, with the servers at
the remote site listed as failover equivalents for the remote site. With
the passthrough configuration, this ensures that each message is
delivered to each remote site exactly once. Whichever server at the
remote site receives the message distributes it to the other server
using passthrough replication.

With this configuration, publishers at each site publish to the primary
local AMPS instance, and subscribers subscribe to messages from their
local AMPS instances. Both publishers and subscribers use the high
availability features of the AMPS client libraries to ensure that if the
primary local instance AMPS fails, they automatically failover to the
other instance. Replication is used to deliver both high availability
and disaster recovery. In the table below, each row represents a
replication destination. Servers in brackets are represented as sets of
``InetAddr`` elements in the ``Destination`` definition.

+--------------------------------------+--------------------------------------+
| Server                               | Destinations                         |
+======================================+======================================+
| Chicago 1                            | * sync to Chicago 2                  |
|                                      |                                      |
|                                      | * async to [NewYork 1, NewYork 2]    |
|                                      |                                      |
|                                      | * async to [London 1, London 2]      |
|                                      |                                      |
+--------------------------------------+--------------------------------------+
| Chicago 2                            | * sync to Chicago 1                  |
|                                      |                                      |
|                                      | * async to [NewYork 1, NewYork 2]    |
|                                      |                                      |
|                                      | * async to [London 1, London 2]      |
|                                      |                                      |
+--------------------------------------+--------------------------------------+
| NewYork 1                            | * sync to NewYork 2                  |
|                                      |                                      |
|                                      | * async to [Chicago 1, Chicago 2]    |
|                                      |                                      |
|                                      | * async to [London 1, London 2]      |
+--------------------------------------+--------------------------------------+
| NewYork 2                            | * sync to NewYork 1                  |
|                                      |                                      |
|                                      | * async to [Chicago 1, Chicago 2]    |
|                                      |                                      |
|                                      | * async to [London 1, London 2]      |
+--------------------------------------+--------------------------------------+
| London 1                             | * sync to London 2                   |
|                                      |                                      |
|                                      | * async to [Chicago 1, Chicago 2]    |
|                                      |                                      |
|                                      | * async to [NewYork 1, NewYork 2]    |
+--------------------------------------+--------------------------------------+
| London 2                             | * sync to London 1                   |
|                                      |                                      |
|                                      | * async to [Chicago 1, Chicago 2]    |
|                                      |                                      |
|                                      | * async to [NewYork 1, NewYork 2]    |
+--------------------------------------+--------------------------------------+

**Table 25.1:** *Geographic Replication with HA Destinations*

.. include:: ./replication.inc
.. include:: ./ha.inc
.. include:: ./queue_replication.inc
